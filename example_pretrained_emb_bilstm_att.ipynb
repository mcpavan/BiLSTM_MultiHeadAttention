{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:10.859968Z",
     "start_time": "2020-05-18T17:44:06.035320Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brMoral Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:10.930686Z",
     "start_time": "2020-05-18T17:44:10.860891Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../../data/BRMoral/brmoral.csv\"\n",
    "mf_data_path = \"../../data/BRMoral/mf-ternary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset and splitting by task (*ap*, *mf* or *st*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.118570Z",
     "start_time": "2020-05-18T17:44:10.931683Z"
    }
   },
   "outputs": [],
   "source": [
    "dfcorpus = pd.read_csv(data_path, na_values=['na'], sep=';', encoding = \"ISO-8859-1\")\n",
    "dfcorpus_mf = pd.read_csv(mf_data_path, na_values=['na'], sep=';', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.189385Z",
     "start_time": "2020-05-18T17:44:11.119572Z"
    }
   },
   "outputs": [],
   "source": [
    "text_cols = []\n",
    "concat_text = \"\"\n",
    "ap_classes = []\n",
    "st_classes = []\n",
    "mf_classes = []\n",
    "\n",
    "\n",
    "for col in dfcorpus.columns:\n",
    "    if col.startswith(\"t.\"):\n",
    "        text_cols.append(col)\n",
    "    elif col.startswith(\"ap.\"):\n",
    "        ap_classes.append(col)\n",
    "    elif col.startswith(\"st.\"):\n",
    "        st_classes.append(col)\n",
    "    elif col.startswith(\"mf.\"):\n",
    "        mf_classes.append(col)\n",
    "    elif \"concat\" in col:\n",
    "        concat_text = col\n",
    "    else:\n",
    "        ap_classes.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.289119Z",
     "start_time": "2020-05-18T17:44:11.190358Z"
    }
   },
   "outputs": [],
   "source": [
    "ap_corpus = dfcorpus[[concat_text] + ap_classes].dropna()\n",
    "mf_corpus = dfcorpus_mf[[concat_text] + mf_classes].dropna()\n",
    "st_corpus = dfcorpus[text_cols + st_classes].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the number of instances and features of each task dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.362897Z",
     "start_time": "2020-05-18T17:44:11.290116Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ap_corpus: {ap_corpus.shape}\")\n",
    "print(f\"mf_corpus: {mf_corpus.shape}\")\n",
    "print(f\"st_corpus: {st_corpus.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the values of each class for Author Profilling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.450662Z",
     "start_time": "2020-05-18T17:44:11.363894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for classe in ap_classes:\n",
    "    print(f\"{ap_corpus.groupby(classe).count()[concat_text]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the values of each class for Moral Fundaments task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.534438Z",
     "start_time": "2020-05-18T17:44:11.451659Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for classe in mf_classes:\n",
    "    print(f\"{mf_corpus.groupby(classe).count()[concat_text]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the values of each class for Stance task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:11.631204Z",
     "start_time": "2020-05-18T17:44:11.536432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for classe in st_classes:\n",
    "    print(f\"{st_corpus.groupby(classe).count()[text_cols[0]]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:17.815435Z",
     "start_time": "2020-05-18T17:44:11.632201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:18.216712Z",
     "start_time": "2020-05-18T17:44:17.816437Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:19.921379Z",
     "start_time": "2020-05-18T17:44:18.217711Z"
    }
   },
   "outputs": [],
   "source": [
    "txt_aux = {}\n",
    "for col in tqdm_notebook(text_cols):\n",
    "    txt_aux[col] = []\n",
    "    for text in dfcorpus[col]:\n",
    "        tokenized_text = nltk.word_tokenize(text, language=\"Portuguese\")\n",
    "        txt_aux[col].append(len(tokenized_text))\n",
    "\n",
    "s = pd.DataFrame(txt_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:20.099901Z",
     "start_time": "2020-05-18T17:44:19.922376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.describe(percentiles=[0.01*i for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the last 100 words of each text, we have few losses of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the StratifiedKFold to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:20.233543Z",
     "start_time": "2020-05-18T17:44:20.100900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training LSTM for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:36.609774Z",
     "start_time": "2020-05-18T17:44:20.234540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from py_util.bilstm_attention import LSTM_Network\n",
    "from py_util.preprocessing import PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:36.961832Z",
     "start_time": "2020-05-18T17:44:36.610772Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filtered_feat_tgt(feat, tgt, train_idx, test_idx):\n",
    "    train_features = pd.Series(feat)[train_idx]\n",
    "    train_targets = pd.Series(tgt)[train_idx]\n",
    "\n",
    "    test_features = pd.Series(feat)[test_idx]\n",
    "    test_targets = pd.Series(tgt)[test_idx]\n",
    "\n",
    "    return train_features, train_targets, test_features, test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the general function to train the NNs for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:37.265021Z",
     "start_time": "2020-05-18T17:44:36.962829Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "             attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "             batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "             suffix=\"\", rdn_ste_split=None, test_size=None, save_preproc_root=None, pretr_emb_path=None):\n",
    "\n",
    "    att_name = f'_attn_{attn_depth_model}_{attn_num_heads}' if attention else ''\n",
    "    suffix = suffix if suffix.startswith(\"_\") or suffix==\"\" else f\"_{suffix}\"\n",
    "    model_name = f\"bilstm_{embed_dim}_{lstm_layer_size}_early{early_stopping}{att_name}{suffix}\"\n",
    "    \n",
    "    for cls_idx, cls in tqdm_notebook(enumerate(classes_), desc=\"Topics\", total=len(classes_)):\n",
    "        nets[cls] = []\n",
    "        save_preproc_root = save_preproc_root or save_dir_root\n",
    "        save_dir_preproc = f\"{save_preproc_root}/pre_process/seq_{seq_length}/{task_}/{cls}\"\n",
    "        os.makedirs(save_dir_preproc, exist_ok=True)\n",
    "        \n",
    "        if isinstance(text_var_, list):\n",
    "            pre_proc = PreProcess(corpus_[text_var_[cls_idx]], corpus_[cls],\n",
    "                                  seq_length, save_dir=save_dir_preproc, pretr_emb_path=pretr_emb_path)\n",
    "        else:\n",
    "            pre_proc = PreProcess(corpus_[text_var_], corpus_[cls],\n",
    "                                  seq_length, save_dir=save_dir_preproc, pretr_emb_path=pretr_emb_path)\n",
    "        \n",
    "        if n_folds and n_folds > 1:\n",
    "            k_folf_gen = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=rdn_ste_split)\n",
    "            iterator_ = enumerate(k_folf_gen.split(pre_proc.features, corpus_[cls]))\n",
    "\n",
    "            for k, (train_idx, test_idx) in tqdm_notebook(iterator_, desc=\"Fold\", total=n_folds):\n",
    "                save_dir = f\"{save_dir_root}/{model_name}/{task_}/{cls}/cv{k}/\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "                log_file = open(f\"{save_dir}/log.txt\", mode=\"w\")\n",
    "                print(f\"\\n\\n{cls} - Fold {k}\", file=log_file)\n",
    "\n",
    "                fit_fold(cls, attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers,\n",
    "                         lstm_layer_size,pre_proc, save_dir, batch_size, early_stopping, learning_rate, log_file, n_epochs,\n",
    "                         random_state_valid, train_print, valid_size, nets, train_idx=train_idx, test_idx=test_idx)\n",
    "        else:\n",
    "            trn_tst_spl = train_test_split(pre_proc.features,\n",
    "                                           pre_proc.vec_targets,\n",
    "                                           test_size=test_size,\n",
    "                                           random_state=rdn_ste_split,\n",
    "                                           stratify=pre_proc.vec_targets)\n",
    "\n",
    "            save_dir = f\"{save_dir_root}/{model_name}/{task_}/{cls}/\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            log_file = open(f\"{save_dir}/log.txt\", mode=\"w\")\n",
    "            print(f\"\\n\\n{cls}\", file=log_file)\n",
    "\n",
    "            fit_fold(cls, attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size,\n",
    "                     pre_proc, save_dir, batch_size, early_stopping, learning_rate, log_file, n_epochs, random_state_valid,\n",
    "                     train_print, valid_size, nets, trn_tst_spl=trn_tst_spl)\n",
    "        del pre_proc\n",
    "    \n",
    "def fit_fold(cls, attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size,\n",
    "             pre_proc, save_dir, batch_size, early_stopping, learning_rate, log_file, n_epochs,\n",
    "             random_state, train_print, valid_size, nets=None, train_idx=None, test_idx=None, trn_tst_spl=None):\n",
    "    gc.collect()\n",
    "    lstm_net = LSTM_Network(pre_proc         = pre_proc,\n",
    "                            attention        = attention,\n",
    "                            attn_depth_model = attn_depth_model,\n",
    "                            attn_num_heads   = attn_num_heads,\n",
    "                            bi_dir           = True,\n",
    "                            dropout_prob     = dropout_prob,\n",
    "                            embed_dim        = embed_dim,\n",
    "                            n_rec_layers     = n_rec_layers,\n",
    "                            lstm_layer_size  = lstm_layer_size,\n",
    "                            save_dir         = save_dir)\n",
    "    \n",
    "    if trn_tst_spl:\n",
    "        train_features, test_features, train_targets, test_targets = trn_tst_spl\n",
    "    else:\n",
    "        results = get_filtered_feat_tgt(feat = pre_proc.features,\n",
    "                                        tgt  = pre_proc.vec_targets,\n",
    "                                        train_idx = train_idx,\n",
    "                                        test_idx  = test_idx)\n",
    "        train_features, train_targets, test_features, test_targets = results\n",
    "    \n",
    "    lstm_net.fit(train_x          = list(train_features),\n",
    "                 train_y          = list(train_targets),\n",
    "                 batch_size       = batch_size,\n",
    "                 early_stopping   = early_stopping,\n",
    "                 learning_rate    = learning_rate,\n",
    "                 log_file         = log_file,\n",
    "                 num_epochs       = n_epochs,\n",
    "                 random_state     = random_state,\n",
    "                 show_every_epoch = train_print,\n",
    "                 test_x           = list(test_features),\n",
    "                 test_y           = list(test_targets),\n",
    "                 valid_size       = valid_size)\n",
    "    lstm_net.destroy_graph()\n",
    "    del lstm_net, train_features, train_targets, test_features, test_targets\n",
    "#     nets[cls].append(lstm_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T03:32:52.881150Z",
     "start_time": "2020-05-18T03:32:52.597908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task Parameters\n",
    "task_ = \"ap\"\n",
    "corpus_ = ap_corpus\n",
    "classes_ = ap_classes\n",
    "text_var_ = concat_text\n",
    "nets = ap_nets = {}\n",
    "suffix = \"skipgram_folha_train_test_split\"\n",
    "save_dir_root = \"D:/USP/Mestrado/Stance/checkpoints/brMoral\"\n",
    "save_preproc_root = f\"{save_dir_root}/preproc_skipgram_folha\"\n",
    "pretr_emb_path = \"D:/USP/Mestrado/Stance/checkpoints/embeddings/w2v_skipgram\"\n",
    "\n",
    "#Split Parameters\n",
    "n_folds = None\n",
    "rdn_ste_split = 123\n",
    "test_size = 0.2\n",
    "\n",
    "#Pre processing parameters\n",
    "seq_length = 800\n",
    "\n",
    "#LSTM Parameters\n",
    "dropout_prob = 0.5\n",
    "embed_dim = 128\n",
    "n_rec_layers = 1\n",
    "lstm_layer_size = 64\n",
    "\n",
    "#Attention parameters\n",
    "attention = True\n",
    "attn_depth_model = 32\n",
    "attn_num_heads = 1\n",
    "\n",
    "#Train Parameters\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_print = True\n",
    "early_stopping = 10\n",
    "random_state_valid = 42\n",
    "valid_size = 0.25\n",
    "\n",
    "\n",
    "# fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "#          attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "#          batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "#          suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T16:01:54.248555Z",
     "start_time": "2020-05-18T03:53:13.343851Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_layer_size_list = [\n",
    "    8,\n",
    "    16,\n",
    "#     32,\n",
    "#     64,\n",
    "    128\n",
    "]\n",
    "\n",
    "attn_depth_model_list = [\n",
    "    8,\n",
    "    16,\n",
    "    32\n",
    "]\n",
    "\n",
    "pretr_emb_path_list = [\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s100.txt'  , \"nilc_cbow_100_train_test_split\"   , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s1000.txt' , \"nilc_cbow_s1000_train_test_split\" , 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s300.txt'  , \"nilc_cbow_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s50.txt'   , \"nilc_cbow_s50_train_test_split\"   , 50  ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s100.txt' , \"nilc_glove_s100_train_test_split\" , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s1000.txt', \"nilc_glove_s1000_train_test_split\", 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s300.txt' , \"nilc_glove_s300_train_test_split\" , 300),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s600.txt' , \"nilc_glove_s600_train_test_split\" , 600),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s100.txt'  , \"nilc_skip_s100_train_test_split\"  , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s1000.txt' , \"nilc_skip_s1000_train_test_split\" , 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s300.txt'  , \"nilc_skip_s300_train_test_split\"  , 300 ),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s600.txt'  , \"nilc_skip_s600_train_test_split\"  , 600 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s50.txt'   , \"nilc_skip_s50_train_test_split\"   , 50  )\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "prod = tqdm_notebook(list(product(lstm_layer_size_list, attn_depth_model_list, pretr_emb_path_list))[24:], desc=\"Combination\")\n",
    "for lstm_layer_size, attn_depth_model, (pretr_emb_path, suffix, embed_dim) in prod:fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "             attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "             batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "             suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size,\n",
    "             save_preproc_root=save_preproc_root, pretr_emb_path=pretr_emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MF (Ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:37.550261Z",
     "start_time": "2020-05-18T17:44:37.267015Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task Parameters\n",
    "task_ = \"mf\"\n",
    "corpus_ = mf_corpus\n",
    "classes_ = mf_classes\n",
    "text_var_ = concat_text\n",
    "nets = mf_nets = {}\n",
    "suffix = \"skipgram_folha_train_test_split\"\n",
    "save_dir_root = \"D:/USP/Mestrado/Stance/checkpoints/brMoral\"\n",
    "save_preproc_root = f\"{save_dir_root}/preproc_skipgram_folha\"\n",
    "pretr_emb_path = \"D:/USP/Mestrado/Stance/checkpoints/embeddings/w2v_skipgram\"\n",
    "\n",
    "#Split Parameters\n",
    "n_folds = None\n",
    "rdn_ste_split = 123\n",
    "test_size = 0.2\n",
    "\n",
    "#Pre processing parameters\n",
    "seq_length = 800\n",
    "\n",
    "#LSTM Parameters\n",
    "dropout_prob = 0.5\n",
    "embed_dim = 128\n",
    "n_rec_layers = 1\n",
    "lstm_layer_size = 64\n",
    "\n",
    "#Attention parameters\n",
    "attention = True\n",
    "attn_depth_model = 32\n",
    "attn_num_heads = 1\n",
    "\n",
    "#Train Parameters\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_print = True\n",
    "early_stopping = 10\n",
    "random_state_valid = 42\n",
    "valid_size = 0.25\n",
    "\n",
    "\n",
    "# fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "#          attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "#          batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "#          suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T09:56:05.103405Z",
     "start_time": "2020-05-18T17:45:36.731911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_layer_size_list = [\n",
    "    8,\n",
    "    16,\n",
    "#     32,\n",
    "#     64,\n",
    "    128\n",
    "]\n",
    "\n",
    "attn_depth_model_list = [\n",
    "    8,\n",
    "    16,\n",
    "    32\n",
    "]\n",
    "\n",
    "pretr_emb_path_list = [\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s100.txt'  , \"nilc_cbow_100_train_test_split\"   , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s1000.txt' , \"nilc_cbow_s1000_train_test_split\" , 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s300.txt'  , \"nilc_cbow_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s50.txt'   , \"nilc_cbow_s50_train_test_split\"   , 50  ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s100.txt' , \"nilc_glove_s100_train_test_split\" , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s1000.txt', \"nilc_glove_s1000_train_test_split\", 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s300.txt' , \"nilc_glove_s300_train_test_split\" , 300),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s600.txt' , \"nilc_glove_s600_train_test_split\" , 600),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s100.txt'  , \"nilc_skip_s100_train_test_split\"  , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s1000.txt' , \"nilc_skip_s1000_train_test_split\" , 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s300.txt'  , \"nilc_skip_s300_train_test_split\"  , 300 ),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s600.txt'  , \"nilc_skip_s600_train_test_split\"  , 600 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s50.txt'   , \"nilc_skip_s50_train_test_split\"   , 50  )\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "prod = tqdm_notebook(list(product(lstm_layer_size_list, attn_depth_model_list, pretr_emb_path_list))[3:], desc=\"Combination\")\n",
    "for lstm_layer_size, attn_depth_model, (pretr_emb_path, suffix, embed_dim) in prod:\n",
    "    fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "             attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "             batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "             suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size,\n",
    "             save_preproc_root=save_preproc_root, pretr_emb_path=pretr_emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:31:58.526015Z",
     "start_time": "2020-05-16T21:31:58.241771Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task Parameters\n",
    "task_ = \"st\"\n",
    "corpus_ = st_corpus\n",
    "classes_ = st_classes\n",
    "text_var_ = text_cols\n",
    "nets = st_nets = {}\n",
    "suffix = \"skipgram_folha_train_test_split\"\n",
    "save_dir_root = \"D:/USP/Mestrado/Stance/checkpoints/brMoral\"\n",
    "save_preproc_root = f\"{save_dir_root}/preproc_skipgram_folha\"\n",
    "pretr_emb_path = \"D:/USP/Mestrado/Stance/checkpoints/embeddings/w2v_skipgram\"\n",
    "\n",
    "#Split Parameters\n",
    "n_folds = None\n",
    "rdn_ste_split = 123\n",
    "test_size = 0.2\n",
    "\n",
    "#Pre processing parameters\n",
    "seq_length = 100\n",
    "\n",
    "#LSTM Parameters\n",
    "dropout_prob = 0.6\n",
    "embed_dim = 128\n",
    "n_rec_layers = 1\n",
    "lstm_layer_size = 64\n",
    "\n",
    "#Attenrion parameters\n",
    "attention = True\n",
    "attn_depth_model = 32\n",
    "attn_num_heads = 1\n",
    "\n",
    "#Train Parameters\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_print = True\n",
    "early_stopping = 10\n",
    "random_state_valid = 42\n",
    "valid_size = 0.25\n",
    "\n",
    "\n",
    "# fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "#          attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "#          batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "#          suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T14:27:33.373247Z",
     "start_time": "2020-05-16T06:02:47.792940Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_layer_size_list = [\n",
    "    8,\n",
    "    16,\n",
    "#     32,\n",
    "#     64,\n",
    "    128\n",
    "]\n",
    "\n",
    "attn_depth_model_list = [\n",
    "    8,\n",
    "    16,\n",
    "    32\n",
    "]\n",
    "\n",
    "pretr_emb_path_list = [\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s100.txt'  , \"nilc_cbow_100_train_test_split\"   , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s1000.txt' , \"nilc_cbow_s1000_train_test_split\" , 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s300.txt'  , \"nilc_cbow_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s50.txt'   , \"nilc_cbow_s50_train_test_split\"   , 50  ),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s100.txt' , \"nilc_glove_s100_train_test_split\" , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s1000.txt', \"nilc_glove_s1000_train_test_split\", 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s300.txt' , \"nilc_glove_s300_train_test_split\" , 300),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s600.txt' , \"nilc_glove_s600_train_test_split\" , 600),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s100.txt'  , \"nilc_skip_s100_train_test_split\"  , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s1000.txt' , \"nilc_skip_s1000_train_test_split\" , 1000),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s300.txt'  , \"nilc_skip_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s600.txt'  , \"nilc_skip_s600_train_test_split\"  , 600 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s50.txt'   , \"nilc_skip_s50_train_test_split\"   , 50  )\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "prod = tqdm_notebook(list(product(lstm_layer_size_list, attn_depth_model_list, pretr_emb_path_list))[9:], desc=\"Combination\")\n",
    "for lstm_layer_size, attn_depth_model, (pretr_emb_path, suffix, embed_dim) in prod:\n",
    "    fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "             attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "             batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "             suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size,\n",
    "             save_preproc_root=save_preproc_root, pretr_emb_path=pretr_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T03:06:56.282544Z",
     "start_time": "2020-05-16T21:32:29.990161Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_layer_size_list = [\n",
    "    8,\n",
    "    16,\n",
    "#     32,\n",
    "#     64,\n",
    "    128\n",
    "]\n",
    "\n",
    "attn_depth_model_list = [\n",
    "    8,\n",
    "    16,\n",
    "    32\n",
    "]\n",
    "\n",
    "pretr_emb_path_list = [\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s100.txt'  , \"nilc_cbow_100_train_test_split\"   , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s1000.txt' , \"nilc_cbow_s1000_train_test_split\" , 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s300.txt'  , \"nilc_cbow_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\cbow_s50.txt'   , \"nilc_cbow_s50_train_test_split\"   , 50  ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s100.txt' , \"nilc_glove_s100_train_test_split\" , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s1000.txt', \"nilc_glove_s1000_train_test_split\", 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s300.txt' , \"nilc_glove_s300_train_test_split\" , 300),\n",
    " ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\glove_s600.txt' , \"nilc_glove_s600_train_test_split\" , 600),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s100.txt'  , \"nilc_skip_s100_train_test_split\"  , 100 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s1000.txt' , \"nilc_skip_s1000_train_test_split\" , 1000),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s300.txt'  , \"nilc_skip_s300_train_test_split\"  , 300 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s600.txt'  , \"nilc_skip_s600_train_test_split\"  , 600 ),\n",
    "#  ('D:/USP/Mestrado/Stance/checkpoints/embeddings/NILCtemp\\\\skip_s50.txt'   , \"nilc_skip_s50_train_test_split\"   , 50  )\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "prod = tqdm_notebook(list(product(lstm_layer_size_list, attn_depth_model_list, pretr_emb_path_list)), desc=\"Combination\")\n",
    "for lstm_layer_size, attn_depth_model, (pretr_emb_path, suffix, embed_dim) in prod:\n",
    "    fit_task(task_, corpus_, classes_, text_var_, nets, seq_length, n_folds,\n",
    "             attention, attn_depth_model, attn_num_heads, dropout_prob, embed_dim, n_rec_layers, lstm_layer_size, save_dir_root,\n",
    "             batch_size, early_stopping, learning_rate, n_epochs, random_state_valid, train_print, valid_size,\n",
    "             suffix=suffix, rdn_ste_split=rdn_ste_split, test_size=test_size,\n",
    "             save_preproc_root=save_preproc_root, pretr_emb_path=pretr_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T05:38:55.563781Z",
     "start_time": "2019-11-13T05:37:55.281010Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T06:11:01.001052Z",
     "start_time": "2019-11-13T05:38:55.564754Z"
    }
   },
   "outputs": [],
   "source": [
    "! shutdown -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stance]",
   "language": "python",
   "name": "conda-env-stance-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
